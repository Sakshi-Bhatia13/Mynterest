{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8952389,"sourceType":"datasetVersion","datasetId":5387658}],"dockerImageVersionId":30096,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tune ","metadata":{}},{"cell_type":"markdown","source":"## Step 1. Data preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport json\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:50:42.414975Z","iopub.execute_input":"2024-07-14T19:50:42.415331Z","iopub.status.idle":"2024-07-14T19:50:42.520032Z","shell.execute_reply.started":"2024-07-14T19:50:42.415300Z","shell.execute_reply":"2024-07-14T19:50:42.519301Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def cleaning(s):\n    s = str(s)\n    s = re.sub('\\s\\W',' ',s)\n    s = re.sub('\\W,\\s',' ',s)\n    s = re.sub(\"\\d+\", \"\", s)\n    s = re.sub('\\s+',' ',s)\n    s = re.sub('[!@#$_]', '', s)\n    s = s.replace(\"co\",\"\")\n    s = s.replace(\"https\",\"\")\n    s = s.replace(\"[\\w*\",\" \")\n    return s","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:37:21.035191Z","iopub.execute_input":"2024-07-14T19:37:21.035515Z","iopub.status.idle":"2024-07-14T19:37:21.041552Z","shell.execute_reply.started":"2024-07-14T19:37:21.035486Z","shell.execute_reply":"2024-07-14T19:37:21.040730Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/outfit-descriptions/captions.json', 'r') as file:\n    data = json.load(file)\n\ndf = pd.DataFrame(list(data.items()), columns=['Image', 'Description'])\n\ndf = df.drop(columns=['Image'])\ntext_data = open('Descriptions.txt', 'w')\nfor idx, item in df.iterrows():\n  article = cleaning(item[\"Description\"])\n  text_data.write(article)\ntext_data.close()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:37:21.045253Z","iopub.execute_input":"2024-07-14T19:37:21.045579Z","iopub.status.idle":"2024-07-14T19:37:27.126520Z","shell.execute_reply.started":"2024-07-14T19:37:21.045550Z","shell.execute_reply":"2024-07-14T19:37:27.125369Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Model Training","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:50:54.726901Z","iopub.execute_input":"2024-07-14T19:50:54.727326Z","iopub.status.idle":"2024-07-14T19:51:02.924855Z","shell.execute_reply.started":"2024-07-14T19:50:54.727285Z","shell.execute_reply":"2024-07-14T19:51:02.923699Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.5.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.4.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.59.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.45)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.3.17)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import TextDataset, DataCollatorForLanguageModeling\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom transformers import Trainer, TrainingArguments","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:51:02.926867Z","iopub.execute_input":"2024-07-14T19:51:02.927251Z","iopub.status.idle":"2024-07-14T19:51:02.932676Z","shell.execute_reply.started":"2024-07-14T19:51:02.927196Z","shell.execute_reply":"2024-07-14T19:51:02.931545Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def load_dataset(file_path, tokenizer, block_size = 128):\n    dataset = TextDataset(\n        tokenizer = tokenizer,\n        file_path = file_path,\n        block_size = block_size,\n    )\n    return dataset\n\n\ndef load_data_collator(tokenizer, mlm = False):\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer, \n        mlm=mlm,\n    )\n    return data_collator\n\n\ndef train(train_file_path,model_name,\n          output_dir,\n          overwrite_output_dir,\n          per_device_train_batch_size,\n          num_train_epochs,\n          save_steps):\n  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n  train_dataset = load_dataset(train_file_path, tokenizer)\n  data_collator = load_data_collator(tokenizer)\n\n  tokenizer.save_pretrained(output_dir)\n      \n  model = GPT2LMHeadModel.from_pretrained(model_name)\n\n  model.save_pretrained(output_dir)\n\n  training_args = TrainingArguments(\n          output_dir=output_dir,\n          overwrite_output_dir=overwrite_output_dir,\n          per_device_train_batch_size=per_device_train_batch_size,\n          num_train_epochs=num_train_epochs,\n      )\n\n  trainer = Trainer(\n          model=model,\n          args=training_args,\n          data_collator=data_collator,\n          train_dataset=train_dataset,\n  )\n      \n  trainer.train()\n  trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:51:02.934670Z","iopub.execute_input":"2024-07-14T19:51:02.935041Z","iopub.status.idle":"2024-07-14T19:51:02.944735Z","shell.execute_reply.started":"2024-07-14T19:51:02.935003Z","shell.execute_reply":"2024-07-14T19:51:02.943833Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# you need to set parameters \ntrain_file_path = \"/kaggle/working/Descriptions.txt\"\nmodel_name = 'gpt2'\noutput_dir = '/kaggle/working/result'\noverwrite_output_dir = False\nper_device_train_batch_size = 8\nnum_train_epochs = 5.0\nsave_steps = 500","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:51:17.661615Z","iopub.execute_input":"2024-07-14T19:51:17.661974Z","iopub.status.idle":"2024-07-14T19:51:17.667210Z","shell.execute_reply.started":"2024-07-14T19:51:17.661944Z","shell.execute_reply":"2024-07-14T19:51:17.666384Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# It takes about 30 minutes to train in colab.\ntrain(\n    train_file_path=train_file_path,\n    model_name=model_name,\n    output_dir=output_dir,\n    overwrite_output_dir=overwrite_output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    save_steps=save_steps\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T19:51:19.297162Z","iopub.execute_input":"2024-07-14T19:51:19.297509Z","iopub.status.idle":"2024-07-14T20:34:11.382262Z","shell.execute_reply.started":"2024-07-14T19:51:19.297479Z","shell.execute_reply":"2024-07-14T20:34:11.381279Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89f895b755fb4377b3c3bf6846bf8abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27fc084c05d34e28ac111955c5ffd858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ded71d1c686468aafe4e0baea77e3ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f1d5859b05e4687b899a678411030e9"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b13e096cd8e4651978ebc49cf37a5bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79749420dd9149658bd81d26c85e17e7"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.4 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.26<br/>\n                Syncing run <strong style=\"color:#cdcd00\">/kaggle/working/result</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/vanib31415-abv-iiitm-gwalior/huggingface\" target=\"_blank\">https://wandb.ai/vanib31415-abv-iiitm-gwalior/huggingface</a><br/>\n                Run page: <a href=\"https://wandb.ai/vanib31415-abv-iiitm-gwalior/huggingface/runs/3lx8g02b\" target=\"_blank\">https://wandb.ai/vanib31415-abv-iiitm-gwalior/huggingface/runs/3lx8g02b</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20240714_195336-3lx8g02b</code><br/><br/>\n            "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='5365' max='5365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5365/5365 40:24, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.536300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.414200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.397400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.389500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.381600</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.377100</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.375000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.370900</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.368800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.367000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:64: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 3. Inference","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Assuming `tokenizer` is your tokenizer\ntokenizer_path = \"/kaggle/working/tokenizer\"\ntokenizer.save_pretrained(tokenizer_path)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T21:26:43.246087Z","iopub.execute_input":"2024-07-14T21:26:43.246443Z","iopub.status.idle":"2024-07-14T21:26:43.277621Z","shell.execute_reply.started":"2024-07-14T21:26:43.246404Z","shell.execute_reply":"2024-07-14T21:26:43.276279Z"},"trusted":true},"execution_count":35,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-94c7c2f8780d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Assuming `tokenizer` is your tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/tokenizer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model_path = \"/kaggle/working/result\"\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\nmodel.save_pretrained(output_dir)\n# tokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:52:48.235131Z","iopub.execute_input":"2024-07-14T20:52:48.235633Z","iopub.status.idle":"2024-07-14T20:52:54.065695Z","shell.execute_reply.started":"2024-07-14T20:52:48.235592Z","shell.execute_reply":"2024-07-14T20:52:54.064751Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-07-14T20:53:23.638591Z","iopub.execute_input":"2024-07-14T20:53:23.638944Z","iopub.status.idle":"2024-07-14T20:53:23.644865Z","shell.execute_reply.started":"2024-07-14T20:53:23.638913Z","shell.execute_reply":"2024-07-14T20:53:23.643802Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def load_model(model_path):\n    model = GPT2LMHeadModel.from_pretrained(model_path)\n    return model\n\n\ndef load_tokenizer(tokenizer_path):\n    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n    return tokenizer\n\n\ndef generate_text(sequence, max_length, n):\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n    \n    model_path = \"/kaggle/working/result\"\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    prompts = []\n\n    for _ in range(n):\n        ids = tokenizer.encode(sequence, return_tensors='pt')\n        final_outputs = model.generate(\n            ids,\n            do_sample=True,\n            max_length=max_length,\n            pad_token_id=tokenizer.eos_token_id,\n            top_k=50,\n            top_p=0.95,\n            eos_token_id=tokenizer.eos_token_id  # Ensure generation stops at EOS token\n        )\n        prompt = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n        \n        # Post-processing to ensure the prompt ends with a complete sentence\n        prompt = prompt.rstrip()  # Remove any trailing whitespace\n        if not prompt.endswith(('.', '!', '?')):  # Check if the prompt doesn't end with punctuation\n            # Find the last punctuation mark to cut off the prompt at the end of the sentence\n            for punctuation in ('.', '!', '?'):\n                pos = prompt.rfind(punctuation)\n                if pos != -1:\n                    prompt = prompt[:pos+1]\n                    break\n        \n        prompts.append(prompt)\n    \n    return prompts","metadata":{"execution":{"iopub.status.busy":"2024-07-14T21:18:05.374280Z","iopub.execute_input":"2024-07-14T21:18:05.374631Z","iopub.status.idle":"2024-07-14T21:18:05.385725Z","shell.execute_reply.started":"2024-07-14T21:18:05.374600Z","shell.execute_reply":"2024-07-14T21:18:05.384634Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Example usage:\nsequence = \"Generate an outfit description based on current fashion trends:\"\nmax_length = 100\nn = 50\noutfit_descriptions = generate_text(sequence, max_length, n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:20:57.890651Z","iopub.execute_input":"2024-07-14T22:20:57.891001Z","iopub.status.idle":"2024-07-14T22:23:56.584664Z","shell.execute_reply.started":"2024-07-14T22:20:57.890971Z","shell.execute_reply":"2024-07-14T22:23:56.583850Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"negative_sequence = \"Generate a poor outfit description based on current fashion trends:\"\nnegative_prompts = generate_text(negative_sequence, max_length, n)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:24:41.291904Z","iopub.execute_input":"2024-07-14T22:24:41.292280Z","iopub.status.idle":"2024-07-14T22:27:37.071411Z","shell.execute_reply.started":"2024-07-14T22:24:41.292236Z","shell.execute_reply":"2024-07-14T22:27:37.070581Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Outfit_Description': outfit_descriptions,\n    'Negative_Prompt': negative_prompts\n})\n\n# Save to CSV\ncsv_path = \"/kaggle/working/outfit_descriptions_with_neg_prompts.csv\"\ndf.to_csv(csv_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:28:02.764937Z","iopub.execute_input":"2024-07-14T22:28:02.765295Z","iopub.status.idle":"2024-07-14T22:28:02.776814Z","shell.execute_reply.started":"2024-07-14T22:28:02.765261Z","shell.execute_reply":"2024-07-14T22:28:02.775807Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}